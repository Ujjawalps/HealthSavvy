{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a92f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31906724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3da548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df333eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the Data from the PDF file\n",
    "def load_pdf_file(data):\n",
    "    loader=DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8006e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_file(data='g:/langchain projects/Med-AI-Gen-AI/Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cba597",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting the Data into Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b13f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of the Text chunks: \", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f540e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(\"Hello World!\")\n",
    "print(\"Length of the query result: \", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd281ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.exists('.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded28238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b19daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# I can also set my API key via environment variable or directly, this one is fine\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists. Skipping creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39320329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833903da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e618485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Exisiting Index\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embeding each chunk and then upserting the embeddings into my Pinecone index\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83feae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is Acne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6011ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configuring the API\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# List of all available models\n",
    "try:\n",
    "    for m in genai.list_models():\n",
    "        if 'generateContent' in m.supported_generation_methods:\n",
    "            print(f\"Name: {m.name}\")\n",
    "            print(f\"Display Name: {m.display_name}\")\n",
    "            print(f\"Description: {m.description}\")\n",
    "            print(\"------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing models: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing basic generation, kaam kar raha hai ya nahi\n",
    "try:\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "    response = model.generate_content('Hello, are you working?')\n",
    "    print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"Error generating content: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f925253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Loading the API key from .env, as I mentioned earlier, me dheere dheere samajhdaar hota ja raha hoon\n",
    "load_dotenv()\n",
    "\n",
    "# Configure the Gemini API kyuki Chatgpt paise maang raha tha, OpenAI is pay to use (what a shame)\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Gemini model ko initiate kar raha hoon, mujhe lagta hai ki yeh mujhe zyada achha response dega\n",
    "# kyuki yeh Google ka hai aur mujhe Google pe zyada trust hai, kyuki mujhe Google ka data chahiye\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0.4,\n",
    "    max_output_tokens=500,\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# Using it like OpenAI, OpenAI ka v shame command hai, mujhe nahi pata kyun\n",
    "response = llm.invoke(\"What are the symptoms of diabetes?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73182df1",
   "metadata": {},
   "source": [
    "#### Niche wala code jada robust and manageable hai but filhal ushe use nahi karunga, project ko lite rakha hai. Will soon feed more data into it and that knowledge base will be managed by this chunk of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7451ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_medical_chat():\n",
    "#     \"\"\"Initialize and configure the medical chat assistant\"\"\"\n",
    "#     from dotenv import load_dotenv\n",
    "#     import os\n",
    "#     import google.generativeai as genai\n",
    "#     from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "#     # Load environment variables\n",
    "#     load_dotenv()\n",
    "#     api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "#     if not api_key:\n",
    "#         raise ValueError(\"GOOGLE_API_KEY not found in environment\")\n",
    "\n",
    "#     # Configure Gemini\n",
    "#     genai.configure(api_key=api_key)\n",
    "\n",
    "#     # Create chat model with medical-specific settings\n",
    "#     llm = ChatGoogleGenerativeAI(\n",
    "#         model=\"gemini-1.5-pro\",\n",
    "#         temperature=0.4,  # Lower temperature for more factual responses\n",
    "#         max_output_tokens=1000,  # Increased for detailed medical responses\n",
    "#         google_api_key=api_key,\n",
    "#         streaming=True  # Enable streaming for faster initial responses\n",
    "#     )\n",
    "\n",
    "#     return llm\n",
    "\n",
    "# # Initialize the medical chat assistant\n",
    "# medical_chat = create_medical_chat()\n",
    "\n",
    "# # Function to ask medical questions\n",
    "# def ask_medical_question(question: str) -> str:\n",
    "#     \"\"\"Ask a medical question and get a response\"\"\"\n",
    "#     try:\n",
    "#         response = medical_chat.invoke(question)\n",
    "#         return response.content\n",
    "#     except Exception as e:\n",
    "#         return f\"Error: {str(e)}\"\n",
    "\n",
    "# # Example usage\n",
    "# question = \"What are the symptoms of diabetes?\"\n",
    "# print(ask_medical_question(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8919cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a medical assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \" If you don't know the answer, just say 'I don't know' do not say anything else other than 'I don't know'. \"\n",
    "    \"Be concise and accurate. You can answer in just 3 sentences maximum. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is the treatment for acne?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Philosophy?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d3a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Acromegaly and gigantism?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
